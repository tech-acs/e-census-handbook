---
sidebar_position: 1
---

## 11.1 Data Processing
Data processing is an integral part of a census process in which various steps are involved to organize, clean, and analyze the collected raw data into usable information that can be analyzed and disseminated. Census data processing is a complex and time-consuming process that requires careful planning, attention to details and execution through use of appropriate tools and methods. 

**Data Processing during the non-digital vs digital era**
In a paper census, data processing steps are time-consuming and can be prone to errors, particularly during the data entry and editing stages. This process is critical to ensure that the data collected is accurate, reliable, and usable for planning and policy-making purposes. Traditional census data processing steps included:

    -	Data collection which included the physical enumeration of households and individuals within a designated geographic area, using paper questionnaires. 
    -	Data entry which involved the manual or electronic entry of the data collected during the census into a computer database or other software. 
    -	Data editing which involved checking the completeness, consistency, and accuracy of the data collected during the census, and making corrections where necessary. 
    -	Data coding which involved assigning codes to the data to facilitate analysis and aggregation, such as coding responses to open-ended questions or assigning codes to geographic locations. 
    -	Data cleaning which involves identifying and correcting errors or inconsistencies in the data, such as missing values or outliers. 
    -	Data aggregation that involves combining the individual responses from the questionnaires into summary statistics, such as population counts by age and gender. 
    -	Data analysis that involves use of statistical software to analyze the data collected during the census, including calculating frequencies, averages, and other statistical measures. 
    -	Data dissemination which involves sharing the processed data with stakeholders, such as government agencies, non-governmental organizations, businesses, and the public, through reports, databases, and other communication channels.
    
In a digital census, data is captured and transmitted electronically, and real-time data validation is undertaken to streamline and improve the accuracy of data processing procedure. Therefore, three main phases are followed systematically:

Stage 1: Pre-enumeration - data processing starts here with development of CAPI application where consistency checks are developed and included in the data collection applications for enumeration and cartographic mapping. Data processing is also done during cartographic mapping to produce the required EA maps. During this stage, the consistency checks are developed as part of the questionnaire specifications for CAPI development. Prior to data processing, editing and imputation specifications and tabulation plans are supposed to be developed. Development of this documents is done by the subject matter specialists together with data processors/programmers. It is advisable to test these tools using the pilot data .

Stage 2: Enumeration - In a digital census, data collection is done and transmitted electronically which saves time and improves data quality through skipping some steps like manual data entry and editing. Monitoring of data quality during data collection is done through supervisor re-interviews and consistency checks, quality checks by a team of data analysts and subject matter specialists as well as real time monitor of enumeration process using dashboards . During monitoring, the identified inconsistencies are sent to the field team to make the corrections.

Stage 3: Post-enumeration - After enumeration, data processing entails using machine developed editing and imputation specifications that are used to undertake the data cleaning; data validation; tabulations; data analysis using various specialized tools and specific attributes guided by the subject matter specialists  and development of census products.  
Raw data contain two broad types of errors, those that impede or prevent processing (structural errors) and those that introduce distortions in the data (inconsistencies) without interrupting the logical flow of subsequent processing operations. All structural errors must be corrected, and as many of the inconsistencies as possible. Failure to correct structural errors (such as missing record types, duplicate or missing records, blank records, etc.) will make tabulation impossible.

### 11.1.1	Selected Country Experiences

**Kenya**
>The data processing team embarked on the process of assessing the census enumeration coverage immediately after data collection. This process involved the harmonization of EA Geo-codes that had been shared during tabletsâ€™ assembly and those used during enumeration. There was need for confirmation that all the EAs created during the mapping exercise were accounted for (with or without data). The next step was extraction of data from tablets whose data did not reach the server by close of enumeration . This was followed by checking and confirming whether all the EAs had data both from pre-enumeration listing of households and enumeration data. A comparison of listed households and actual returns, and accounting for the variances was done.  In addition, a comparison of mapped households, listed households and actual enumerated households was also done and any >discrepancies addressed.
>
>The next step was to check for data consistency at household and individual levels. Basic edit specifications were developed to check the inconsistencies. The purpose of editing was to ensure that the final dataset did not include training data (data that was collected during training of census personnel) as well as ineligible individuals (empty records). In addition, the editing process took the following into account:  households with missing household heads; individual records without age, sex, and relationship for conventional households; individual records without sex and age for non- conventional households; and duplicate records for structure, household numbers, and enumerator codes . Subsequently, tabulation plans were developed and used to generate summary tables for basic and analytical reports. Data in the generated tables was compared with existing data sets to assess the consistency of enumeration coverage and the outputs were validated by key stakeholders and thereafter launched.
>
>Census data editing: Kenya utilized the handbook on Population and Housing Census Editing to come up with the editing specifications for the different modules in the census; Census data editing Team: The editing team was composed of data processors, subject matter specialists (demographic and non-demographic), computer programmers, supervisors and managers; KNBS has the inhouse capacity to program the edit specifications and run them to generate clean data files; We also utilized experienced CSPro data processors (former KNBS staff) who helped in the programming of editing/ imputation codes. Analytical Reports:  Subject matter specialists drawn from the 2019 KPHC drafted the reports and then shared with the stakeholders for review; KNBS received TA support through the UNFPA ESARO office to come up with Population Projections report; Looking forward to receiving TA to come up with Geo-Spatial Dissemination System. Data Quality checks: An edit specification document should be prepared by the subject matter specialists and shared before pilot is undertaken; Hard checks in CAPI should be enforced for majority of the questions to reduce content errors.

### 11.1.2	Lessons Learnt
Some of the challenges experienced during data processing included:

    i)	Discrepancies in the number of EAs from enumeration and data validation.
    
    ii)	Delay in finalization of edit specifications which slowed the harmonization of Geocodes. 

### 11.1.3	Recommendations
Recommendations for future censuses
i)	Ensure edit specifications are in place on time. To obtain accurate and hence useful census results, data must be free, to the greatest extent possible, from errors and inconsistencies, especially after the data processing stage.
ii)	Improve on how enumerators are trained to minimize errors. 
iii)	The tabulation plans to be alongside the development of data collection tools.

